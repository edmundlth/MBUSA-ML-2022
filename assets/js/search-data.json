{
  
    
        "post0": {
            "title": "Week 5 Written Notes on Whiteboard",
            "content": "",
            "url": "https://edmundlth.github.io/MBUSA-ML-2022/pen-and-paper/machine-learning/boosting/adaboost/xgboost/gradient-boosting/2022/08/31/week5-written-notes.html",
            "relUrl": "/pen-and-paper/machine-learning/boosting/adaboost/xgboost/gradient-boosting/2022/08/31/week5-written-notes.html",
            "date": " • Aug 31, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Week 5 notebook",
            "content": "Written notes . Link to things written on whiteboard . import numpy as np import sklearn import matplotlib.pyplot as plt . Sampling with replacement for a given probability distribution . def sample_one(probabilities): &quot;&quot;&quot; Generate a single sample with probability density specified by `probabilities = [p1, p2, ..., pN]` We will return an index `i` between 0 and N-1 inclusive. &quot;&quot;&quot; assert np.abs(np.sum(probabilities) - 1) &lt; 0.0001 # just checking this is an honest probability density N = len(probabilities) # size of the sample space cummulative_density = np.cumsum(probabilities) # compute CDF cummulative_density = np.concatenate([[0], cummulative_density]) # prepend 0 to the list random_seed = np.random.rand() # generate a single random number between 0 and 1 uniformly. # Look for the index i such that CDF[i -1] &lt;= random_seed &lt;= CDF[i] for i in range(N): if cummulative_density[i] &lt; random_seed &lt;= cummulative_density[i + 1]: return i # break and return when found def sample(n, items, probabilities): &quot;&quot;&quot;For pedagogy only. Inefficient implementation&quot;&quot;&quot; samples = [] for _ in range(n): index = sample_one(probabilities) samples.append(items[index]) return samples items = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;] probabilities = [2/3, 1/6, 1/6] num_samples = 10000 samples = sample(num_samples, items, probabilities) print(f&quot;Empirical probabilities:&quot;) a, b = np.unique(samples, return_counts=True) for a, b in zip(*np.unique(samples, return_counts=True)): print(f&quot;Probability of oberving {a} = {b / num_samples}&quot;) . Empirical probabilities: Probability of oberving a = 0.6647 Probability of oberving b = 0.1629 Probability of oberving c = 0.1724 . Adaboost walkthrough . Generating dataset. . from sklearn.datasets import make_circles from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier from sklearn.inspection import DecisionBoundaryDisplay from sklearn.ensemble import VotingClassifier from sklearn.linear_model import LogisticRegression n = 1000 sigma = 0.3 factor = 0.1 X, y = make_circles(n_samples=n, noise=sigma, factor=factor) y = (2 * y - 1).astype(int) plt.plot(X[y == 1][:, 0], X[y == 1][:, 1], &#39;b+&#39;) plt.plot(X[y == -1][:, 0], X[y == -1][:, 1], &#39;r+&#39;) . [&lt;matplotlib.lines.Line2D at 0x12568b070&gt;] . Using one weak learner. . clf = DecisionTreeClassifier(max_depth=1) # clf = LogisticRegression(C=1e5) clf.fit(X, y) fig, ax = plt.subplots(1, 1, figsize=(10, 10)) DecisionBoundaryDisplay.from_estimator(clf, X, alpha=0.2, response_method=&quot;predict&quot;, ax=ax) ax.plot(X[y == 1][:, 0], X[y == 1][:, 1], &#39;b+&#39;) ax.plot(X[y == -1][:, 0], X[y == -1][:, 1], &#39;r+&#39;) . [&lt;matplotlib.lines.Line2D at 0x1257d5790&gt;] . Boosting to train a few learners. . classifiers = [] weights = np.ones(n) / n confidences = [] loss_fn = lambda f, x, y: np.exp(-y * f(x)) num_iter = 54 chosen = [] weights_rec = [] for t in range(num_iter): weights = weights / np.sum(weights) weights_rec.append(weights) # clf = SVC(kernel=&quot;linear&quot;, C=1.0) clf = DecisionTreeClassifier(max_depth=1) # clf = LogisticRegression(C=1e5) indices = sample(n, list(range(len(y))), weights) X_t = X[indices] y_t = y[indices] chosen.append(indices) clf.fit(X_t, y_t) classifiers.append(clf) pred = clf.predict(X) error = np.sum([weights[i] for i in range(n) if pred[i] != y[i]]) # error = np.clip(np.sum(pred != y), a_min=0.01, a_max=0.99) conf = 1/2 * np.log((1 - error) / error) confidences.append(conf) weights = weights * np.exp(-(pred * y) * conf) . What those individual learners did. . N = min(15, num_iter) ncol = 3 nrow = N // ncol + (N % ncol) fig, axes = plt.subplots(nrow, ncol, figsize=(5 * ncol, 5 * nrow)) axes = np.ravel(axes) for i in range(N): ax = axes[i] clf = classifiers[i] indices = chosen[i] weights = 5 ** (np.array(weights_rec[i][indices]) * n) # print(weights) ax.scatter( X[indices][y[indices] == 1][:, 0], X[indices][y[indices] == 1][:, 1], color=&quot;blue&quot;, marker=&quot;*&quot;, s=weights[y[indices] == 1] ) ax.scatter( X[indices][y[indices] == -1][:, 0], X[indices][y[indices] == -1][:, 1], color=&quot;red&quot;, marker=&quot;*&quot;, s=weights[y[indices] == -1] ) DecisionBoundaryDisplay.from_estimator(clf, X, alpha=0.2, response_method=&quot;predict&quot;, ax=ax) . Weighted majority voting of weak learners. . Question: Write down the decision rule. . fig, ax = plt.subplots(1, 1, figsize=(10, 10)) ax.scatter( X[y == 1][:, 0], X[y == 1][:, 1], # s=10 ** (weights[y == 1] * 100), marker=&quot;*&quot;, color=&quot;blue&quot; ) ax.scatter( X[y == -1][:, 0], X[y == -1][:, 1], # s=10 ** (weights[y == -1] * 100), marker=&quot;*&quot;, color=&quot;red&quot; ) a = np.linspace(-2, 2, num=50) b = np.linspace(-2, 2, num=50) ps = np.array([(x1, x2) for x1 in a for x2 in b]) preds = np.array([clf.predict(ps) for clf in classifiers]).T preds = preds * np.array(confidences) pos = np.sum(preds, axis=1) neg = np.sum(-preds, axis=1) Z = 2 * (pos &gt; neg) - 1 Z = Z.reshape(50, 50) XX, YY = np.meshgrid(a, b) ax.contourf(XX, YY, Z, alpha=0.2) . &lt;matplotlib.contour.QuadContourSet at 0x12609cb20&gt; . Scikit-learn Adaboost implementation . from sklearn.ensemble import AdaBoostClassifier clf = AdaBoostClassifier( # base_estimator=SVC(kernel=&quot;linear&quot;), algorithm=&quot;SAMME&quot;, n_estimators=54 ) clf.fit(X, y) fig, ax = plt.subplots(1, 1, figsize=(10, 10)) DecisionBoundaryDisplay.from_estimator(clf, X, alpha=0.2, response_method=&quot;predict&quot;, ax=ax) ax.plot(X[y == 1][:, 0], X[y == 1][:, 1], &#39;b+&#39;) ax.plot(X[y == -1][:, 0], X[y == -1][:, 1], &#39;r+&#39;) . [&lt;matplotlib.lines.Line2D at 0x12621ec10&gt;] . Bagging vs Boosting . Bagging Boosting . Sampling | Uniformly at random. | Weighted based on performance from last iteration. | . Parallelisable? | Can train learners in parallel | Learners training depend on previous iteration. | . Classification rule | Majority vote | Weighted majority vote | . Bias or Variance | Focus on reducing variance | Focus on reducing bias | . Prone to overfitting? | Relatively safe from overfitting | Can overfit | . | . XGBoost demo walkthrough . x = np.linspace(0, 10, num=20) y = np.ones_like(x) y[(x &gt; 3) &amp; (x &lt; 6)] *= 5 y[(x &gt; 6)] *= 10 y += np.random.randn(len(x)) * 0.8 x = x.reshape(-1, 1) plt.plot(x, y, &quot;b*&quot;) . [&lt;matplotlib.lines.Line2D at 0x1263438b0&gt;] . from xgboost import XGBRegressor, XGBClassifier xgboostmodel = XGBRegressor( n_estimators=10, eta=1, # min_child_weight=1, max_depth=1, # max_leaves=2, # reg_alpha=10, # reg_lambda=100 ) xgboostmodel.fit(x, y) plt.plot(x, y, &quot;b*&quot;) plt.plot(x, xgboostmodel.predict(x), &quot;b--&quot;) . [&lt;matplotlib.lines.Line2D at 0x1293a9a00&gt;] . Residue . plt.plot(x, y - xgboostmodel.predict(x), &quot;b*&quot;) . [&lt;matplotlib.lines.Line2D at 0x129715df0&gt;] .",
            "url": "https://edmundlth.github.io/MBUSA-ML-2022/machine-learning/workshop/boosting-method/adaboost/gradient-boosting/xgboost/2022/08/31/week5-notebook.html",
            "relUrl": "/machine-learning/workshop/boosting-method/adaboost/gradient-boosting/xgboost/2022/08/31/week5-notebook.html",
            "date": " • Aug 31, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Week 4 Written Notes and Exercise Solutions",
            "content": "",
            "url": "https://edmundlth.github.io/MBUSA-ML-2022/pen-and-paper/machine-learning/2022/08/24/week4-written-notes.html",
            "relUrl": "/pen-and-paper/machine-learning/2022/08/24/week4-written-notes.html",
            "date": " • Aug 24, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Week 4 notebook",
            "content": "import numpy as np import matplotlib.pyplot as plt import pandas as pd from ipywidgets import interact import torch . . . Important: Run this notebook in the cloud by clicking any of the options at the top of the page (e.g. google colab). . Recommended Educational Videos related to this topic . These are a series of videos by 3blue1brown on youtube that I HIGHLY RECOMMEND for understanding neural networks with pretty animations: . Introducing neural network: . . Gradient descent: . . Backpropagation to effectively calculate gradient: . . Details about backpropagation: . . Some visualisation . Activation functions . . Important: Run this notebook in the cloud by clicking any of the options at the top of the page (e.g. google colab). The following are visualisations of some common activation function used in ANN. . ReLU (rectified linear unit). Just a fancy name for $ mathrm{ReLU}(x) = max(0, x)$ | Sigmoid: $ sigma(x) = 1 / (1 + e^{-x})$ | tanh: hyperbolic tangent. | . @interact(w=(-3, 3), b=(-3, 3)) def visualise(w=1, b=0, activation=[&quot;relu&quot;, &quot;tanh&quot;, &quot;sigmoid&quot;]): x = np.linspace(-5, 5, num=100) s = w * x + b if activation == &quot;relu&quot;: y = np.clip(s, a_min=0, a_max=np.inf) elif activation == &quot;tanh&quot;: y = np.tanh(s) elif activation == &quot;sigmoid&quot;: y = 1 / (1 + np.exp(s)) plt.plot(x, y) . Composing activation functions at different parameters . This is an interactive visualisation of a neural network implementing the following function: $$ f_w(x) = sigma(w_1 sigma(v_1 x + a_1) + w_2 sigma(v_2 x + a_2) + b) $$ which is a fully connected (feedforward) network with . 1 input node. | 2 node in the hidden layer. | 1 output node. | . You can count that, . from input to first hidden layer, there are $ 1 times 2 + 2 = 4$ parameters, they are $(v_1, v_2, a_1, a_2)$. | from first hidden layer to output layer, there are $2 times 1 + 1 = 3$ parameters, they are $(w_1, w_2, b)$. | total num parameters $ = 4 + 3 = 7$, they are $w = (v_1, v_2, a_1, a_2, w_1, w_2, b)$. | . Play with the slider to change the paramters and see the changes to $f_w$. How closely Can you get $f_w$ (blue curve) to match the given orange curve? . @interact(w1=(-3., 3.), w2=(-3., 3.), w3=(-3., 3.), w4=(-3., 3.), b1=(-3., 3.), b2=(-3., 3.), b3=(-3., 3.), b4=(-3., 3.)) def visualise_1_hidden_layer(w1=1, w2=1, w3=0, w4=0, b1=1, b2=0, b3=0, b4=0, activation=[&quot;relu&quot;, &quot;tanh&quot;, &quot;sigmoid&quot;]): x = np.linspace(-5, 5, num=100) s1 = w1 * x + b1 s2 = w2 * x + b2 if activation == &quot;relu&quot;: f = lambda s: np.clip(s, a_min=0, a_max=np.inf) elif activation == &quot;tanh&quot;: f = lambda s: np.tanh(s) elif activation == &quot;sigmoid&quot;: f = lambda s: 1 / (1 + np.exp(s)) a1 = f(s1) a2 = f(s2) y = f(w3 * a1 + b3 + w4 * a2 + b4) plt.plot(x, y) plt.plot(x, -x**2) . Visualise gradient descent path . This might be a better representation for what SGD does in more complicated (therefore closer to reality) scenario than what was usually shown in a 1D-parameter case with unique minima. . The countours in the picture shows the &quot;loss landscape&quot;, meaning it is the contour plot of some loss function $L(w_1, w_2)$ with 2 parameters -- $(w_1, w_2)$ are the axes. | The lighter the colour of the contour, the higher the loss (the worse the model is at the particular $(w_1, w_2)$. The goal of any optimisation algorithm -- like SGD -- is to &quot;slide&quot; down to a low loss region (darker red region). | The red dots are various random initialisation. | The colour trajectories starting from the red dots are SGD trajectories. Their end point would the output of the optimisation algorithm (i.e. the training output). | . Question: . Could there be multiple optima? | Could there be multiple global optima? | Do the trajectories always find an optima? | Do the trajectories always find a global optima? | What happen if we increases the learning rate $ eta$? | What happen if we stop the trajectories earlier? | . . . Finding mean using gradient descent . . Important: Run this notebook in the cloud by clicking any of the options at the top of the page (e.g. google colab). This is an illustration of a (very) simple machine learning problem, where the goal is to find out an unknown mean $ mu^*$ of a Gaussian that gives us an i.i.d. data set $D_n = {x_1, x_2, dots, x_n }$, where the $x_i$ are drawn from normal distribution $N( mu^*, sigma=3)$. . Let&#39;s generate the data first. (Change n as you like. Try a low n ~ 5 first and then a high n ~ 100 later). We shall plot the randomly generated data on a histogram. Does the data look like a Gaussian for you choice of n? . true_mean = 2 # would be nice if we know this. sigma = 3 n = 10 x = sigma * np.random.randn(n) + true_mean plt.hist(x) . (array([1., 0., 1., 1., 3., 0., 2., 0., 1., 1.]), array([-5.23554849, -3.74630945, -2.2570704 , -0.76783135, 0.7214077 , 2.21064675, 3.69988579, 5.18912484, 6.67836389, 8.16760294, 9.65684199]), &lt;BarContainer object of 10 artists&gt;) . Gradient descent algorithm for finding empirical mean . We will now develop a gradient descent algorithm to estimate the mean. We first define a loss function $$ ell( mu) = frac{1}{n} sum_{i = 1}^n ( mu - x_i)^2. $$ . Why do we choose this &quot;sum-of-square-error&quot; loss function? Well, because it has a (global) minimum that coincide with the empirical mean $$ hat{ mu} = frac{1}{n} sum_{i = 1}^n x_i = mathrm{argmin}_ mu ell( mu). $$ . So, if we gradient descent against this loss, we will find $ hat{ mu}$, and hopefully that is a good estimate of $ mu^*$. Remember, $ mu^*$, the thing we don&#39;t know, is our actual goal, $ hat{ mu}$ is but a good guess of what $ mu^*$ is . . We plot the &quot;loss landscape&quot; below. Observe that the minimum (which we shall find using gradient descent later), might not line up with the true mean. . Question: What if you increase your data set size to n &gt; 100? . def loss(mu, x): return 1/2 * np.sum((x - mu)**2) mu_range = np.linspace(-5, 5, num=100) plt.plot(mu_range, [loss(mu, x) for mu in mu_range]) . [&lt;matplotlib.lines.Line2D at 0x135e9aac0&gt;] . Here we perform a few steps of gradient descent: . The blue curve is the loss function given the data points. | The red line and stars represent the gradient descent trajectories. The endpoint of this trajectory is our guess $ tilde{ mu}$. | The blue star is the empirical mean, $ hat{ mu}$. | The green star is the true mean (the $ mu^*$ that generated the data in the first place). | Notice that (particularly if you pick a low n), $ tilde{ mu}$ might not equal $ hat{ mu}$, because the optimisation might not actually find the global minimum. | $ hat{ mu}$ might not equal $ mu^*$, because the empirical mean is subjected to sampling error. | . | . def grad(mu, x): return np.sum(mu - x) def step(mu, x, rate=0.001): return mu - rate * grad(mu, x) num_epoch = 5 learning_rate = 0.05 mu = -0.5 #np.random.rand() mus = [mu] for e in range(num_epoch): mu = step(mu, x, rate=learning_rate) mus.append(mu) fig, ax = plt.subplots(1, 1, figsize=(10, 10)) mu_range = np.linspace(np.min(mus), 3, num=100) ax.plot(mu_range, [loss(mu, x) for mu in mu_range]) ax.plot(mus, [loss(mu, x) for mu in mus], &#39;r*&#39;, linestyle=&quot;dotted&quot;, markersize=15) ax.plot([true_mean], loss(true_mean, x), &#39;g*&#39;, label=&quot;true mean&quot;, markersize=15) ax.plot([np.mean(x)], loss(np.mean(x), x), &#39;b*&#39;, label=&quot;empirical mean&quot;, markersize=15) . [&lt;matplotlib.lines.Line2D at 0x13626c520&gt;] .",
            "url": "https://edmundlth.github.io/MBUSA-ML-2022/machine-learning/workshop/neural-network/sgd/2022/08/24/week4-notebook.html",
            "relUrl": "/machine-learning/workshop/neural-network/sgd/2022/08/24/week4-notebook.html",
            "date": " • Aug 24, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Q&A (Week 3)",
            "content": "Under the Hard margin SVM objective, it says “parameter b is optimised indirectly by influencing constraints”. Is this “optimisation of parameter b” referring to separation hyperplane or regularisation? . Both w​ and b​ are parameters of SVM model (we need both to specify a hyperplane) and training SVM means finding a good set of (w, b)​ such that some objective function is maximised under some constraints. In hard margin SVM, that means maximise margin subject to the constraints of no misclassification. It turns out that the “maximise margin” part can be expressed purely in terms of w​, and b​ only comes into making sure that the “no misclassification” constraint is satisfied. . I know the slides uses the word “regularisation” in describing the objective of SVM, but I think that is a little confusing. Just think of the 1/ | w | term as an objective function to measure how well a particular choice of (w, b)​ performs for a given training data. | . | Also under the Hard margin SVM objective, what is the “data” here when it says “data-independent regularisation term”? I thought the data was training data, but then I think $1/||w||$ is related to training data, since scaling changes (yi*(wxi+b)). I am confused about why it is considered a “data-independent regularisation term” after scaling the closest distance to 1/||w||. . That slide is just trying to express the fact that once the constraints is satisfied , maximising the 1/ | w | objective no longer cares about training data. It is the constraints of “no misclassification” that depends on the training data. So the training process can be thought of (not actual implementation though) as first using training data to find all (w, b)​ that satisfies the y_i (w x_i + b) &gt;= 1​ constraints and then find the smallest | w | in that set. | . | Under the Lagrange duality, where does lambda comes from w, and how it got to the model here. . This is indeed a whole topic on its own. I think this article explains the origin of the dual variable fairly well. You might want to go through the first section carefully and simultaneously having some easy example in your mind. E.g. try to go through the article with the following optimisation problem: . minimise x^2 subject to -x + 3 &lt;= 0 . As you mentioned, this might be a little out of scope of this subject. . | In the Kernel example, what does the transformation in the last line of the calculation mean? . Notice that it is a function that takes the feature vector (x_1, x_2) with only 2 dimensions and produce another vector with 6 dimensions which includes a (scaled) copy of the original (x1, x2) component but also other quadratic components. In practice, this means we transform our original dataframe with 2 columns into a new dataframe with 6 columns and it is this new dataframe that we feed into SVM so that SVM can do linear​​ separation in this larger 6 dimensional space. . But we can do away with actually computing this new dataframe and feeding a much larger dataframe into SVM by using kernel​​. The observation is that we only ever need to compute dot product of the new 6-dimensional vector, not the vectors themselves. And we observe in this calculation that calculating the dot product in transformed space is the same as evaluating the kernel in the original space, which is much cheaper. . | What is an infinite dimensional vector space? . It is what it says it is, a vector space with infinite dimension. To make sense of this you would need to know that definition of vector spaces and their dimension. Roughly, a (real) vector space is any set of things that you know how to add, and you know how to scale with a real number. The dimension of the vector space is the number of “coordinates” needed to specify a vector in this set. 2-tuples forms a 2D vector space, 3-tuples forms 3D etc.. You can perhaps imagine an infinite list of numbers forming infinite dimensional space (ask yourself: do you know how to add two such lists? do you know how to scale one such list by a real number? If so, then you likely have a vector space). . A non-trivial example relevant to things we discussed is that any (sufficiently nice) function can be expressed by their Taylor series $f(x) = a_0 + a_1 x + a_2 x^2 + dots$ . You can think of this as using an infinite list of numbers $(a_0, a_1, a_2, dots)$ to specify a function. For example, the list $(1, 2, 1, 0, 0, 0, …)$ specify the function that computes $f(x) = 1 + 2x + x^2$. And to specify all such functions, you do need an infinite list, so this is vector space is infinite dimensional.. . I am skipping over lots of details here, but this is mostly covered in any linear algebra course / book. . |",
            "url": "https://edmundlth.github.io/MBUSA-ML-2022/questions/machine-learning/support-vector-machine/2022/08/17/week3-q-and-a.html",
            "relUrl": "/questions/machine-learning/support-vector-machine/2022/08/17/week3-q-and-a.html",
            "date": " • Aug 17, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Week 3 notebook",
            "content": "SVM - demo . . Important: Run this notebook in the cloud by clicking any of the options at the top of the page (e.g. google colab). . Run the following cell and play around with the slide bar to construct your own hyperplane with &quot;maximum margin&quot;. The red line correspond to the &quot;true&quot; margin (used to generate the data, and you should never know what that is in practice). . Question: . Does your solution (orange) match up with the red line? Put another way, is the red line (truth) always the maximum margin hyperplane given the data? | What is the effect of increasing n? i.e. if we collect more data, does the optimised SVM boundary line up better with the true (red) boundary when n is large? | . import numpy as np import matplotlib.pyplot as plt from ipywidgets import interact . . w0 = -1.5 # y = w0 * x + b0 b0 = 1 xaxis = np.linspace(0, 1, num=10) max_n = 100 all_data = np.random.rand(max_n, 2) all_y = (all_data[:, 1] &gt; all_data[:, 0] * w0 + b0) @interact(w=(-3., 3.), b=(-3., 3.), n=(10, 100)) def plot(w=-0.5, b=0.5, n=10): data = all_data[:n, :] y = all_y[:n] fig, ax = plt.subplots(1, 1, figsize=(8, 8)) ax.plot(data[y, 0], data[y, 1], &quot;r*&quot;, markersize=15) ax.plot(data[~y, 0], data[~y, 1], &quot;b*&quot;, markersize=15) ax.set_xlim(0, 1) ax.set_ylim(0, 1) ax.plot(xaxis, w0 * xaxis + b0, &#39;r--&#39;) ax.plot(xaxis, w * xaxis + b, color=&quot;orange&quot;) .",
            "url": "https://edmundlth.github.io/MBUSA-ML-2022/machine-learning/workshop/support-vector-machine/2022/08/17/week3-notebook.html",
            "relUrl": "/machine-learning/workshop/support-vector-machine/2022/08/17/week3-notebook.html",
            "date": " • Aug 17, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Common Themes in Machine Learning",
            "content": "One goal of this note is to help you navigate the terminology you might encounter is machine learning. People give names to things because the name refer to something important but also common enough pattern that we don’t need to come up with lots of names. Below we will see one illustrative common pattern. This is not the full scope of modern machine learning but it is illustrative of lots of common elements. . The task of learning . There is usually a hidden function that we are trying to learn. . y=f(x)y = f(x)y=f(x) . and we try our best to come out with $ hat{f}$ that does that. . We might be doing regression, in which case, we want to answer: given values of a independent variable $x$, what could the response $y = f(x)$ be? . Or we could be doing classification where we want $ hat{f}(x)$ to output the correct class of the observation $x$. . Or we could be trying to approximate the probability of class = some class label given observation $x$. . Common element of machine learning: . Data . We don’t have access to the truth $f$, but we do have access to a list of data $D_N = {(x_1, y_1), dots, (x_N, y_N)}$ and we hope that the data is representative of the function, meaning it is not concentrated on one small region of $x$ for example. . We have lots of names for $x$ - predictors, features, input, measurements… And lots of names for $y$ - response, labels, output, value… . $x$ can be a vector, $x = (x_j)_j = (x_1, x_2, dots, x_m)$, which means sometimes you might see actual numbers being labeled by two indices: . xi=(xij)j=(xi1,xi2,…,xim)x_i = (x_{ij})_j = (x_{i1}, x_{i2}, dots, x{im})xi​=(xij​)j​=(xi1​,xi2​,…,xim) where $x_{ij} =$ value of feature $j$ for data $i$. For example: . $y=$ whether or not it rains and $x$ is cloudiness. So we have one continuous features and one binary output. | $y=$ whether or not it rains and $x$ is cloudiness and whether it rained yesterday. We have one binary output, one continuous feature and one binary feature. | $y=$ probability of rain and $x$ is temperature and cloudiness. We have a continuous output and two dimensional continuous inputs. | … | . Furthermore, we have noise in our data, meaning we only have . yi=f(xi)+ϵiy_i = f(x_i) + epsilon_iyi​=f(xi​)+ϵi​ . for each of the pair in $D_N$, not $y_i = f(x_i)$. This is one way statistics comes into learning. So not only do not know what $f$ is, we don’t even know that the given $y_i$ in the dataset is exactly right. . Model . This of this as family of possible guesses of $ hat{f}$. And they are usually parametrised, so we write $ hat{f}_ theta$. . The parameter $ theta$ determines one particular guess of the estimate $f$. Parameters itself can live in a huge space for example $ theta = ( theta_1, dots, theta_d)$. . Examples: . Naive Bayes. The parameter space depends on the distribution you choose for the factors. For example, in Gaussian Naive Bayes, you still need to find the mean and variance for each $x_i$ and for each class $k$. | Neural networks. The parameters are the network weights. | Decision tree. Parameters are what to split on and where. This is a case where thinking about the set of estimator $ hat{f}$ itself (the tree) is actually easier than thinking about the parameters. | Random Forest. Same parameters as decision trees but one set of parameter for each tree in the forest. | … | . Notice that in the above, there are still other choices to make: distributions for the factors in naive bayes; architecture / topology, depth, width for neural networks; maximum number of nodes or maximum depth for decision trees; total number of trees in random forest. These choices that are made before training are called hyperparameters. . Objective / Score / Loss / Error function . Not every guess in the family of ${ hat{f}}$ will be good and we need a way to measure how good a guess $ hat{f}$ is. For example, . Square loss: $L( hat{f})(x) = (f(x) - hat{f}(x))^2$. | Absolute error: $L( hat{f})(x) = lvert f(x) - hat{f}(x) rvert$. | Cross-entropy | log-loss | . (some of the above can only applied for specific kinds of $f$, e.g. log-loss is when we are trying to predict probability). Wait, but we just say that we don’t have access to $f$! So we can’t actually evaluate any of the above. But we do have access to data where $f(x_i)$ is approximately $y_i$. So we replace that with . L(f^)(xi)=(yi−f^(x))2L( hat{f})(x_i) = (y_i - hat{f}(x))^2L(f^​)(xi​)=(yi​−f^​(x))2 . But we have lots of data points ${(x_i, y_i)}$, well, just sum them up! . L(f^)=∑i(yi−f^(x))2L( hat{f}) = sum_i (y_i - hat{f}(x))^2L(f^​)=i∑​(yi​−f^​(x))2 . and we have arrived at the training error for a particular guess $ hat{f}$. . But our guesses $ hat{f}$ are actually parametrised by $ theta$, so we can actually express training loss as a function of $ theta$, . L(f^θ)=L(θ)=∑i(yi−f^θ(x))2L( hat{f}_ theta) = L( theta) = sum_i (y_i - hat{f}_ theta(x))^2L(f^​θ​)=L(θ)=i∑​(yi​−f^​θ​(x))2 . Training / fitting . With what we have set up above, training a model translate to . Find the best parameter $ theta$ that minimises training loss $L( theta)$. . This becomes purely a problem in optimisation. . Testing and generalisation error . Once a good parameter and therefore a good $ hat{f}$ is found, we can ask . “Ok, so I know that $ theta$ do well in my dataset, but how do I know that it will do well in the real world?” . Ideally we can just compare $ hat{f}_ theta$ against the truth $f$ and account for all possible value of $x$ (by taking expectation for example) L(f^)=∑all possible values of x(f^θ(x)−f(x))2dxL( hat{f}) = sum_{ text{all possible values of $x$}} ( hat{f}_ theta(x) - f(x))^2 dxL(f^​)=∑all possible values of x​(f^​θ​(x)−f(x))2dx (I have ignored the fact that this should be an integral and you might want to weight against distribution of $x$ to make things converge). This is the generalisation score / loss / error. . However, again, this whole exercise only occur because we don’t know $f$!! What we can do is . Go and collect more data and try $ hat{f}_ theta$ on them and recalculate the loss. | Don’t use all the data in $D_N$ for training, reserve some to simulate “collecting more data later on”. (train-test split) | Do the previous train-test split, except we do that multiple times and then average the resulting scores. (cross validations) The hope is that the performance measure generated with these methods will be a good predictor for the actual performance “in real life”. | .",
            "url": "https://edmundlth.github.io/MBUSA-ML-2022/summary/machine-learning/2022/08/17/ml-common-themes.html",
            "relUrl": "/summary/machine-learning/2022/08/17/ml-common-themes.html",
            "date": " • Aug 17, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Week 2 Written Notes and Exercise Solutions",
            "content": "",
            "url": "https://edmundlth.github.io/MBUSA-ML-2022/pen-and-paper/machine-learning/2022/08/10/week2-written-exercise.html",
            "relUrl": "/pen-and-paper/machine-learning/2022/08/10/week2-written-exercise.html",
            "date": " • Aug 10, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Q&A (Week 2)",
            "content": "In terms of hyperparameters, I think you mentioned that X=(X1, X2, …) is part of hyperparameters. Does this mean the train-test split is part of the hyperparameter? . Sorry that confused you. I did not meant to imply that X = (X1, X2, …) are hyperparameters, they are not parameters of any kind. Depending on what your notation meant, they are either features, i.e. each observation (data you collect) constitute a row in a data frame of the form (X, y) with X = (X1, X2, …, Xn) being the independent variables, i.e. the features. . I was giving the example that one could do “feature selection” in which one chooses a subset of N best features according to some criteria (e.g. correlation of X_i with the response y). The size of that subset, N, can be a hyperparameter. Meaning it is something you choose BEFORE you call model.fit()​. If your model is a linear model, then the model itself has parameters (the linear coefficients in this case). Each parameter will produce a different “guess” or “hypothesis” that explains the relationship between X and y and the role of model.fit()​ is to find the best parameter according to a given goodness criterion (accuracy, mean squared error etc). Each different hyperparameter (e.g. N above) will also produce a different hypothesis whose goodness can be measure with any given data. BUT, it is NOT optimised (or “trained”) by model.fit()​. It is instead chosen before calling .fit()​ and usually changes which set of parameter .fit()​ can even affect. In that sense, it is a parameter that control other parameters, hence the name “hyperparameter”. . | Regarding the K-fold stratified cross-validation, some sources mention it improves accuracy. Is this referring to higher accuracy on the training set, or higher accuracy for the test set as well? Cross-validation (of any sort. K-fold, leave-one-out, random sampling) is a method to evaluate​​ out-of-sample accuracy (or any other performance measures). It doesn’t, by itself improves anything. However, by doing cross-validation, one can get information about performance of the model in the future. So, if you do cross-validation across different classifiers, then you can pick the one with the best cross-validated scores to ensure the final classifiers you choose has the best accuracy among all the classifiers you tested. . Also, to use exaggerated language, we NEVER evaluate performance on training set. While we do evaluate accuracy or other performance scores on training set, it is for the purpose of “fitting” the model: to find a parameter, configuration (e.g. decision tree branches) that do well to explain the training data. The score is used for this search, but we never believe that the scores computed during training will be indicative of future performance. We always evaluate how well our model might do on future data – how well our model “generalise” – using data it has never seen during training. This out-of-sample data set can either be a test set (data set aside specifically for this purpose) or we go through a cross-validation process. . | For the decision tree, I think in class we were told to stop splitting when “instances at root have the same class”. Do we need to consider whether to split the continuous features before or after the categorical features when constructing the decision tree? (since taking a different threshold may affect the number of classes after the split, affect the Information Gain calculated, and that may affect when we stop splitting). Or does the threshold or order have no impact on the final model? . That is indeed a subtle thing. Let’s recall what we do for the case where all the features are categorical and the response y is also categorical: at every node, we split on the feature that has the highest information gain. In this case, the ordering of features doesn’t matter since we evaluate InfoGain on every feature and take the best. . Things becomes slightly complicated when continuous variables are involved, but the overall story doesn’t really change: We still construct a way to evaluate “goodness of split” for each feature, evaluate this on all feature and then split on the best. The only that changed is how we evaluate this “goodness of split” on continuous features. So, the order still doesn’t matter, we still evaluate on everything and pick the best. . How we evaluate goodness of split depends on the type of feature and the type of response: . Categorical input -&gt; categorical output. This is the case discussed in lecture and workshop, we evaluate information gain or similar purity criteria. | Categorical input -&gt; continuous output. We can still split on this feature, but now we need to extend the idea of purity that we evaluate on each node to continuous variable. To motivate this a little with an extremely nice situation. We have the following data [(A, 5), (B, 1000), (B, 1050), (A, 7)]. It is clear that when the input category is A, the output is very small and when it is B it is large. The decision rule “if input is A, predict output = 6, else predict 1025”, where those prediction values are the average, this look like a good rule. In general, if we split on the feature values and we use the “average” of the output in each split as our prediction, then we evaluate how good our split is by looking expected error that kind of prediction will make. In our simple example, not splitting on that feature will give the prediction of “5 + 7 + 1000 + 1050 / 4 = 515.5” which is far from all the values we have seen, while if we do split and predict base on A or B, we improve to predicting 6 (which is going to be close to any values in the A branch) and 1025 which is close to any values in the B branch. So in general, we still have a formulate that is similar in spirit to the InfoGain formula: “error of prediction at parent node - average error across children node”. | Continuous input -&gt; categorical output. One way to handle this is to use a threshold on the input value. If we pick a threshold T, then splitting based on whether input &lt; T​ effectively turn this into case 1 above: categorical (binary) input -&gt; categorical output. Its just that there are infinitely many T to choose from. Well, we can just choose the “best T”, meaning find the T where the resulting split has best purity score. And then the goodness of split from that T is then the goodness of split for this feature overall. Choosing the best T from infinitely many might seem impossible, but, this is a very typical situation that just turn into an optimisation problem. E.g. find out the function InfoGain(T) and use calculus to solve for the optimal T. | Continuous input -&gt; continuous output. This is now a regression problem. One way to handle this is to do the same thing as case 2 above, we have a categorical (binary now) input -&gt; continuous output. And the issue of which threshold to choose is again down to issue of optimisation like case 3. | All the above have variation with various strength and weaknesses, but perhaps understand the reasoning behind these methods first might help. See sklearn documentation on decision tree (classifiers and regressors) for reference. . | For ranking-based feature selection of discrete valued features using information gain, is the mutual information calculated based on the original data set (from the top of tree without any split)? . Short answer: Mutual information (or correlation, or any such scores used for feature selection) is evaluated on the “train” data both the train-test split and train-validation-test split. . Long answer: First thing first, feature selection is something you can do regardless of what model you end up using, so this is not about decision tree per se. You might select features you think are best and feed it into DecisionTree, RandomForest, linear models or anything you wish. . The reason we shouldn’t use validation or test data to do feature selection is related to why we don’t use training data to evaluate performance. Feature selection is something you do as part of training: the selected features is going to be used by model.fit()​ to find best parameter. If you use the validation or test data to find such a feature set and then evaluate performance on data you already use to inform how your model is constructed, the resulting performance is going to be overly optimistic. . | I am confused between wrapper-based feature selection and embedded feature selection, as they both seem to have feature selection in parallel to training. Is the main difference that the “mechanism” in embedded feature selection needs to be explicitly defined in the algorithm (like decision tree), while it is considered unknown in the wrapper-based feature selection before the model is trained? . As far as I can tell, your description is correct. The embedded rely on “performance measure” that is built into the model you choose whereas wrapper one is independent on what model you use to evaluate performance. I am not sure these are standard terminology as I have not seen it much … I am going to point you to this site for this question. . | In the Bagging topic, the slide mentioned to use “cross-validation to evaluate its out-of-sample data”. Does it mean that the accuracy will be calculated over the out-of-sample data each time even if the size is different? And when it says bagging “improves unstable classifiers by reducing variance”, I am confused about what this improvement is relative to. Is this talking about improvement in the performance after taking majority vote or averaging compared to the performance from base classifiers which are unstable? . Yes. Remember, the main point of cross-validation is that we evaluate performance on samples not used to train the model. Evaluating on 1 sample the model didn’t see during training is still a way to predict performance, but the value you get out is going to have a high variance compare to evaluating performance on lots of samples and take average. This is just using phenomena typified in the Central limit theorem: if one sample has variance S^2, then averaging over N independent samples has variance S^2 / N. . And yes again, the improvement comes from averaging unstable (high variance) base estimators. Bagging is again using central limit theorem-like phenomena, if you have N independent prediction, you can reduce your prediction variance by 1/N. Although we don’t have guaranteed independence as needed in central limit theorem, we use a lot of tricks (random subsampling training data, random subsampling feature set etc) to make the individual base estimators as “independent” as possible. So, although we won’t get full 1/N reduction in variance, there will still be some​ improvement over individuals. . | Demo implementation of how to do sampling with replacement using different probability density . import numpy as np def sample_one(probabilities): &quot;&quot;&quot; Generate a single sample with probability density specified by `probabilities = [p1, p2, ..., pN]` We will return an index `i` between 0 and N-1 inclusive. &quot;&quot;&quot; assert np.sum(probabilities) # just checking this is an honest probability density N = len(probabilities) # size of the sample space cummulative_density = np.cumsum(probabilities) # compute CDF cummulative_density = np.concatenate([[0], cummulative_density]) # prepend 0 to the list random_seed = np.random.rand() # generate a single random number between 0 and 1 uniformly. # Look for the index i such that CDF[i -1] &lt;= random_seed &lt;= CDF[i] for i in range(N): if cummulative_density[i] &lt; random_seed &lt;= cummulative_density[i + 1]: return i # break and return when found #return N -1 # if loop complete it must have fall on the last interval def sample(n, items, probabilities): &quot;&quot;&quot;For pedagogy only. Inefficient implementation&quot;&quot;&quot; samples = [] for _ in range(n): index = sample_one(probabilities) samples.append(items[index]) return samples items = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;] probabilities = [1/8, 1/8, 4/8, 2/8] num_samples = 10000 samples = sample(num_samples, items, probabilities) print(f&quot;Empirical probabilities:&quot;) a, b = np.unique(samples, return_counts=True) for a, b in zip(*np.unique(samples, return_counts=True)): print(f&quot;Probability of oberving {a} = {b / num_samples}&quot;) Empirical probabilities: Probability of oberving a = 0.1208 Probability of oberving b = 0.1248 Probability of oberving c = 0.505 Probability of oberving d = 0.2494 .",
            "url": "https://edmundlth.github.io/MBUSA-ML-2022/questions/machine-learning/cross-validation/2022/08/10/week2-q-and-a.html",
            "relUrl": "/questions/machine-learning/cross-validation/2022/08/10/week2-q-and-a.html",
            "date": " • Aug 10, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Week 2 workshop discussions",
            "content": ". Workshop plan . We will first finish up NaiveBayes classifier from last week. We will do demo on this, illustrating Bayesian inference and the &quot;naive&quot; assumption. This will hopefully illustrate some aspect of information entropy as well. | Then we will talk about DecisionTreeClassifier. We will discuss this in detail. It is important to understand the &quot;splitting criterion&quot;. We will understand the basic concept of Entropy first. (Q1 written) Demo: number guessing game, probable event encoding. | Entropy as &quot;average surprise&quot;. We want &quot;additive surprise on independent events&quot;. | Entropy as a function of distribution. $H(p)$ | Entropy as a funciton of random variable. $H(X)$ | Entropy of a set. $H(A)$ | Conditional entropy. $H(Y | X)$ | . | Use entropy to define &quot;information gain&quot;. (Q2 written). | . | Next, we talk about &quot;ensembling&quot;. Example: &quot;crowd wisdom&quot;. | The need for classifiers to be &quot;as independent / diverse as possible&quot;. | Bagging: drawing with replacement. | Random Forest classifier. | . | Some coding exercise: Q3, Q4. We will see some &quot;preprocessing&quot; of data. | We will talk about what ML people call &quot;hyperparameters&quot;. Why &quot;hyper&quot;? Hyperparameters in random forest Q5. | . | If time permits, we will talk about: Any question you have. | Overfitting. | Some possibly useful external materials for better understanding. | . | Summary. What do you absolutely need to master from these very diverse materials? | import numpy as np import matplotlib.pyplot as plt import scipy from ipywidgets import interact . . . Bayesian inference demo . Below is a classic demonstration of Bayesian inference. The likelihood we set up will also illustrate the &quot;naive&quot; assumptions used in the NaiveBayes classifier. . Set up . A point $(x^*, y^*)$ is randomly chosen on the unit square $[0, 1] times [0, 1]$. Our task is to find out where that point is based on a sequence of relative direction of randomly generated points on the square. E.g. If $(x^*, y^*) = (0.5, 0.5)$ and the first randomly generated point is $(x_1, y_1) = (0.9, 0.3)$ then we obtain one data point of $SE$ since $(x, y)$ is south east of $(x^*, y^*)$. . Likelihood function . The likelihood of observing $N$ north calls and $E$ east calls in a sequence of $n$ points is given by . $$ begin{align*} L(x, y) &amp;= p(N, E | x^*, y) &amp;= p(N | y) p(E | x) &amp;= binom{n}{N}(1-y)^N y^{n - N} binom{n}{E} (1 - x)^E x^{n - E}. end{align*} $$Notice that . We are assuming that the points are generated uniformly at random. The set up did not tell us that, we are making a modelling assumption here. | In constructing the likelihood function above, we assumed that the North-South and East-West direction are independent. This is not true in general, this is the &quot;naive&quot; assumption, though in this case this follows from the much stronger first assumption above. | The likelihood function is usually thought of as a function of the information we wish to know, in this case the position of the point $P = (x^*, y^*)$, and the likelihood $L(x, y)$ is asking &quot;what is the probability of observing the sequence $ {NE, SE, dots }$ data if $P = (x, y)$&quot;. Crucially, this is NOT the probabiliy of $(x, y)$ in any sense of the word. To get the probability interpretation, we need Bayes formula. | . Bayes formula . $$ p(x, y | N, E) = frac{L(x, y) p(x, y)}{p(N, E)} $$where $p(x, y)$ is a prior and $p(N, E)$ is the evidence (while it is just a normalising constant, it is the term that tell us how likely our modelling assumption is. If the true point generator far from uniform, then evidence is going to be really low). . Demo . Run the follow cell in a Jupyter notebook and give different values of $N$ and $E$ (and control the total $n$ by giving corresponding $S$ and $W$) to see how the posterior changes and how the Maximum Likelihood estimate need not agree with what the posterior says. . def log_likelihood(north, south, east, west, x, y): x_term = east * np.log(1 - x) + west * np.log(x) y_term = north * np.log(1 - y) + south * np.log(y) return x_term + y_term MAX_N = 8 @interact( north=(0, MAX_N), south=(0, MAX_N), east=(0, MAX_N), west=(0, MAX_N), use_prior=True ) def plot_likelihood(north, south, east, west, use_prior): xlist = np.linspace(0.001, 0.999, 100) ylist = np.linspace(0.001, 0.999, 100) X, Y = np.meshgrid(xlist, ylist) Z = np.exp(log_likelihood(north, south, east, west, X, Y)) if use_prior: sigma = 0.5 prior = ( np.exp(-(X - 0.5)**2 / (2 * sigma**2)) * np.exp(-(Y - 0.5)**2 / (2 * sigma**2)) ) else: prior = 1 Z *= prior fig, ax = plt.subplots(1, 1, figsize=(8, 8)) ax.contourf(X, Y, Z, levels=20, cmap=&quot;hot_r&quot;) ax.vlines([0.5], ymin=0, ymax=1, linestyle=&quot;dotted&quot;) ax.hlines([0.5], xmin=0, xmax=1, linestyle=&quot;dotted&quot;) # x_index, y_index = np.unravel_index(np.argmax(Z, axis=None), Z.shape) # x_mle = xlist[x_index] # y_mle = ylist[y_index] x_mle = west / max((east + west), 1) y_mle = south / max(north + south, 1) ax.plot([x_mle], [y_mle], &quot;o&quot;, markersize=10, color=&quot;orange&quot;) ax.set_title(f&quot;Posterior contour. nMaximum Likelihood at: ({x_mle:.2f}, {y_mle:.2f})&quot;) . . Decision tree . An illustration of a &quot;regression tree&quot;: See sklearn docs .",
            "url": "https://edmundlth.github.io/MBUSA-ML-2022/machine-learning/workshop/decision-tree/random-forest/information-theory/2022/08/10/week2-notebook.html",
            "relUrl": "/machine-learning/workshop/decision-tree/random-forest/information-theory/2022/08/10/week2-notebook.html",
            "date": " • Aug 10, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Q&A (Week 1)",
            "content": "Under the Holdout method in the “Sampling for evaluation” section in the worksheet, does the “test set or holdout set” here refer to the validation set or test set? Is validation set relevant to both Holdout method and Cross-validation, or is it only relevant to Cross-validation? . Simple answer: It is referring to the test set. . Complicated answer: Validation set and test set, while referring to different subsets of data, play very similar role. They are both “hold out” or “set aside” part of our full dataset that is exclusively used to find out how well our classifier is performing without using data the classifier has already seen during its training process. . Long answer: The main thing to understand here is that we want to predict how well our classifier will perform and to do that we need to evaluate​​ our classifier on some data. But to get a good estimate of performance, we should not evaluate the model on data it has already seen in the training process because the training process involve trying to do as well as possible on the training data so of course it is expected to do well on them. We need “unseen” / “hold out”/ “set aside” / “out of sample” / “TEST” data. The word “validation” and “test” is basically synonyms, but we use different words just to signal that we are doing the test in slightly different context. See, we might want to know how well our classifier perform for different purposes​​: There is one obvious one: to get a final performance report, the one that you use to boast in a paper, to your boss or to convince customers to use your classifier. This is the number you use to predict how well your classifier will actually perform in the future​​. We often call the set of data that generate this number the “test set”. (but really, this is just convention by this point). One other reason you might want to evaluate performance is to do model selection​​ (notice that KFold​ is implemented in the model_selection​ module of sklearn​). This is the scenario where you have multiple classifier that has already been trained and you want compare​​ their performance. Well, you can evaluate their performance on data not used in any of their training process and choose the classifier that perform best. This set of data used to evaluate performance is often call the “test set” as well. BUT!!​​ This process of selecting the best classifier from multiple classifiers to come up with one ultimate classifier IS PART OF TRAINING​​: it is a process that uses data to find a classifier that we hope perform well in future instance of similar data. To know the actual performance of that selected classifier, we can’t use the same data that we used to select it (same reasoning, it is selected to do well on that data, so of course it will do well). We need to evaluate on another test set. Now we have a linguistic problem of using the same term “test set” referring on two physically different (different values, store in physically different location) thing. So we just change the word to a synonymous one, which is why we call it the Validation set​​. The reason my “simple answer” just says that it is the “test set” is because we haven’t run into this issue of needing to “test” / evaluation for different purpose yet and “test” is the more common / default / general word to use (it is the default word in many ML libraries). . Analogy: We use “mid sem test”, “final test”, “exam”, “quiz” to refer the process of evaluating student performance for different purpose. and different context We could’ve use “model selection testing set” to disambiguate, but “validation” is already widely used. . | Under the Cross-validation evaluation method, when it says “test set” or “testing” in the definitions, does the “test” here refer to validation set? . Following the story we outlined above, “testing” here refer to the process of evaluation classifier performance on data it hasn’t seen during training. We do not need to make the distinction between test / validation here because we are talking about that performance evaluation process in general. . Cross-validation is just a method of splitting full data set in “data to train model” and “data unseen during training” multiple time so that we can train on multiple different training set and evaluate their performance. This is usually known as the “train-test split”. The idea is to get different independently trained model and independently evaluate their performance on independent data so that, when we take average of these independent evaluation, it is a better prediction of actual performance. Average of independent samples from the same distribution is a better prediction of the mean is a basic result from probability, but just how “independent” these evaluation are is up for debate, but we are only applying it as a general wisdom that taking average = good. . | In the Random sub-sampling method, the description says “each train/test split is drawn with replacement from the original data set”. What does the part “with replacement from the original data set” refer to? . Simple answer: This is just one way to come up with the train-test split. And it is talking about a process that comes up with training and testing set using a process like the following. Suppose full data set is of size 10. And you want to get 5 data point in training set and 3 in testing set (exact number not important for now). We can generate 5 random numbers, e.g. [2, 8, 9, 1, 1] and 3 more [5, 7, 8] and put into our training set the data points with index [2, 8, 9, 1, 1] and our test set the data point with index [5, 7, 8]. . Long answer: “Random draw with replacement” is a term used in probability theory. It means we generate a sample of N things by reaching into a bag, randomly picking up something, record what it is and return it into the bag​​, and repeat this N times. Notice that the “replacement” part makes it possible to pick up something you have picked up before (see the index 1 repeated twice above and the index 8 repeated in train and test set). That is a behaviour you would not get if you are doing “random draw without​​ replacement”. Wait ….. isn’t this doing test evaluation on data you might have seen before …… Good point. But if your data set is large enough, that is going to be rare and it turns out that it is not cheating if the distribution​​ of training and testing set is different, even if there might be repeated instances. But you are right though, “sampling without replacement” is a more common practice (most libraries that implement train_test_split, or k-fold cross-validation has a shuffle=true​ option, which is effectively doing sampling without​​ replacement). . How much do you need to care about this? If you haven’t mastered what train-test data are, what “model” means, what K-fold cross validation is, and know a few models and how they are trained and tested and figure out what constitute good “performance measure”, maybe leave this alone first. Just know that this is one option to get train-test set, it is not used particularly often and the theory of why it is valid is perhaps not very important to you right now. . | In the Leave-one-out cross-validation method (see worksheet), it says “n is the number of instances in the data set”. Is this “data set” here referring to the “train and validation set”? . Simple answer: “Data set” refer to full data set, i.e. all the data you have. . Long answer: “Data set” refers to all the data you have to perform training and testing. Remember that we might be talking about testing in different context here. So might not be doing model selection and we just want to train the model and find out how well it does, then we are talking about all the data you have. And if we are talking about a process with selection, then we might already have a testing data set aside for testing the ultimate model, but we might do cross-validation to split the remaining​​ data multiple times, then n = size of remaining data​. . Again, K-Fold, Leave-one-out (which is just K-fold with K selected so that test set is always of size 1) or any such splitting method is there to split any data set, you decide what data set to split and why. . | I might have said something like “ROC is not a score”. Why? What about precision_score, recall_score, and f1_score? . I think I said something like “ROC is not a score like what the score I wrote on the whiteboard meant”. I regretted saying that and I regret picking that word “score” to write on the whiteboard. You might have notice that I use the word “performance” a lot above and I think that is closer to what I meant. Throughout the above questions and answers, I didn’t mention how​​ this performance is to be measured, and indeed that is something we determine case-by-case in practice. The vast majority of the time, we measure performance by picking a score​​: a number that we can compute for a trained classifier on some data with the property that the higher this number is the better the performance is. Accuracy, precision, recall, F1 are all scores in this sense. Other ones that you might be familiar with might include MSE (mean squared error), MAE (mean absolute error) for regression tasks, but be careful that these are numbers that are better the lower​​ they are, so we generally call them “error” or “loss”. . ROC is not a score in the above sense. First of all, it is not even a number, it is a curve on the x-y plane. To make it a number, we generally calculate the area under a ROC curve (AUC). Now this number does have the property that the higher it is, the better something is.. That “something” however is not ONE classifier. Recall that we construct the ROC curve for model that output a number that we can compare with a chosen threshold​​ to decide whether or not an observation is positive or negative. Unfortunately, we call that number a “score” as well, perhaps you can translate it as “confidence”: it is a number that is higher when the model is more sure that it is a positive observation. Upshot is, “trained model + chosen threshold = classifier”, different threshold different classifier. The ROC AUC is a way to find out the performance of the “trained model” part of the equation by looking at its performance across ALL​​ threshold. . So, higher AUC, better model (or more specifically, better TPR-FPR trade off can be made with the model). . |",
            "url": "https://edmundlth.github.io/MBUSA-ML-2022/questions/machine-learning/cross-validation/2022/08/05/week1-q-and-a.html",
            "relUrl": "/questions/machine-learning/cross-validation/2022/08/05/week1-q-and-a.html",
            "date": " • Aug 5, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "External Materials of Interest",
            "content": "Fastai lectures: Lecture 6 about decision trees and random forests | explained.ai Statistics vs ML terms: https://explained.ai/statspeak/index.html | Gradient boosting: https://explained.ai/gradient-boosting/ | . | Bagging vs Boosting on Kaggle https://www.kaggle.com/code/prashant111/bagging-vs-boosting/notebook | Craiyon: https://www.craiyon.com/ | Essay by Leo Breiman (you might know him for the Bagging meta-learning technique): Statistical Modelling: The Two Cultures | Introducing neural network: youtube: https://youtu.be/aircAruvnKk . | Gradient descent: youtube: https://youtu.be/IHZwWFHWa-w . | Backpropagation to effectively calculate gradient: youtube: https://youtu.be/Ilg3gGewQ5U . | Details about backpropagation: youtube: https://youtu.be/tIeHLnjs5U8 . | .",
            "url": "https://edmundlth.github.io/MBUSA-ML-2022/extra/machine-learning/2022/08/05/external-materials-of-interest.html",
            "relUrl": "/extra/machine-learning/2022/08/05/external-materials-of-interest.html",
            "date": " • Aug 5, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "Week 1 Written Notes and Exercise Solutions",
            "content": "",
            "url": "https://edmundlth.github.io/MBUSA-ML-2022/pen-and-paper/machine-learning/2022/08/03/week1-written-notes.html",
            "relUrl": "/pen-and-paper/machine-learning/2022/08/03/week1-written-notes.html",
            "date": " • Aug 3, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "This site collects extra materials used by the Machine Learning module in the Master of Business Analytics course at Melbourne Business School (MBS). Due to copyright reason, course materials won’t be directly posted to this site, but I will attempt to capture a good summary of the topics discussed in class. If you are a student of the course, feel free to ask questions by commenting on each post and I will try to answer. If I take too long to reply (or I just missed it), perhaps emailing me might still be the better option. . This site is generated by fastpages, which is a blogging platform that natively support Jupyter notebooks. .",
          "url": "https://edmundlth.github.io/MBUSA-ML-2022/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://edmundlth.github.io/MBUSA-ML-2022/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}