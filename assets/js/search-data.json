{
  
    
        "post0": {
            "title": "Q&A (Week 1)",
            "content": "Under the Holdout method in the “Sampling for evaluation” section in the worksheet, does the “test set or holdout set” here refer to the validation set or test set? Is validation set relevant to both Holdout method and Cross-validation, or is it only relevant to Cross-validation? . Simple answer: It is referring to the test set. . Complicated answer: Validation set and test set, while referring to different subsets of data, play very similar role. They are both “hold out” or “set aside” part of our full dataset that is exclusively used to find out how well our classifier is performing without using data the classifier has already seen during its training process. . Long answer: The main thing to understand here is that we want to predict how well our classifier will perform and to do that we need to evaluate​​ our classifier on some data. But to get a good estimate of performance, we should not evaluate the model on data it has already seen in the training process because the training process involve trying to do as well as possible on the training data so of course it is expected to do well on them. We need “unseen” / “hold out”/ “set aside” / “out of sample” / “TEST” data. The word “validation” and “test” is basically synonyms, but we use different words just to signal that we are doing the test in slightly different context. See, we might want to know how well our classifier perform for different purposes​​: There is one obvious one: to get a final performance report, the one that you use to boast in a paper, to your boss or to convince customers to use your classifier. This is the number you use to predict how well your classifier will actually perform in the future​​. We often call the set of data that generate this number the “test set”. (but really, this is just convention by this point). One other reason you might want to evaluate performance is to do model selection​​ (notice that KFold​ is implemented in the model_selection​ module of sklearn​). This is the scenario where you have multiple classifier that has already been trained and you want compare​​ their performance. Well, you can evaluate their performance on data not used in any of their training process and choose the classifier that perform best. This set of data used to evaluate performance is often call the “test set” as well. BUT!!​​ This process of selecting the best classifier from multiple classifiers to come up with one ultimate classifier IS PART OF TRAINING​​: it is a process that uses data to find a classifier that we hope perform well in future instance of similar data. To know the actual performance of that selected classifier, we can’t use the same data that we used to select it (same reasoning, it is selected to do well on that data, so of course it will do well). We need to evaluate on another test set. Now we have a linguistic problem of using the same term “test set” referring on two physically different (different values, store in physically different location) thing. So we just change the word to a synonymous one, which is why we call it the Validation set​​. The reason my “simple answer” just says that it is the “test set” is because we haven’t run into this issue of needing to “test” / evaluation for different purpose yet and “test” is the more common / default / general word to use (it is the default word in many ML libraries). . Analogy: We use “mid sem test”, “final test”, “exam”, “quiz” to refer the process of evaluating student performance for different purpose. and different context We could’ve use “model selection testing set” to disambiguate, but “validation” is already widely used. . | Under the Cross-validation evaluation method, when it says “test set” or “testing” in the definitions, does the “test” here refer to validation set? . Following the story we outlined above, “testing” here refer to the process of evaluation classifier performance on data it hasn’t seen during training. We do not need to make the distinction between test / validation here because we are talking about that performance evaluation process in general. . Cross-validation is just a method of splitting full data set in “data to train model” and “data unseen during training” multiple time so that we can train on multiple different training set and evaluate their performance. This is usually known as the “train-test split”. The idea is to get different independently trained model and independently evaluate their performance on independent data so that, when we take average of these independent evaluation, it is a better prediction of actual performance. Average of independent samples from the same distribution is a better prediction of the mean is a basic result from probability, but just how “independent” these evaluation are is up for debate, but we are only applying it as a general wisdom that taking average = good. . | In the Random sub-sampling method, the description says “each train/test split is drawn with replacement from the original data set”. What does the part “with replacement from the original data set” refer to? . Simple answer: This is just one way to come up with the train-test split. And it is talking about a process that comes up with training and testing set using a process like the following. Suppose full data set is of size 10. And you want to get 5 data point in training set and 3 in testing set (exact number not important for now). We can generate 5 random numbers, e.g. [2, 8, 9, 1, 1] and 3 more [5, 7, 8] and put into our training set the data points with index [2, 8, 9, 1, 1] and our test set the data point with index [5, 7, 8]. . Long answer: “Random draw with replacement” is a term used in probability theory. It means we generate a sample of N things by reaching into a bag, randomly picking up something, record what it is and return it into the bag​​, and repeat this N times. Notice that the “replacement” part makes it possible to pick up something you have picked up before (see the index 1 repeated twice above and the index 8 repeated in train and test set). That is a behaviour you would not get if you are doing “random draw without​​ replacement”. Wait ….. isn’t this doing test evaluation on data you might have seen before …… Good point. But if your data set is large enough, that is going to be rare and it turns out that it is not cheating if the distribution​​ of training and testing set is different, even if there might be repeated instances. But you are right though, “sampling without replacement” is a more common practice (most libraries that implement train_test_split, or k-fold cross-validation has a shuffle=true​ option, which is effectively doing sampling without​​ replacement). . How much do you need to care about this? If you haven’t mastered what train-test data are, what “model” means, what K-fold cross validation is, and know a few models and how they are trained and tested and figure out what constitute good “performance measure”, maybe leave this alone first. Just know that this is one option to get train-test set, it is not used particularly often and the theory of why it is valid is perhaps not very important to you right now. . | In the Leave-one-out cross-validation method (see worksheet), it says “n is the number of instances in the data set”. Is this “data set” here referring to the “train and validation set”? . Simple answer: “Data set” refer to full data set, i.e. all the data you have. . Long answer: “Data set” refers to all the data you have to perform training and testing. Remember that we might be talking about testing in different context here. So might not be doing model selection and we just want to train the model and find out how well it does, then we are talking about all the data you have. And if we are talking about a process with selection, then we might already have a testing data set aside for testing the ultimate model, but we might do cross-validation to split the remaining​​ data multiple times, then n = size of remaining data​. . Again, K-Fold, Leave-one-out (which is just K-fold with K selected so that test set is always of size 1) or any such splitting method is there to split any data set, you decide what data set to split and why. . | I might have said something like “ROC is not a score”. Why? What about precision_score, recall_score, and f1_score? . I think I said something like “ROC is not a score like what the score I wrote on the whiteboard meant”. I regretted saying that and I regret picking that word “score” to write on the whiteboard. You might have notice that I use the word “performance” a lot above and I think that is closer to what I meant. Throughout the above questions and answers, I didn’t mention how​​ this performance is to be measured, and indeed that is something we determine case-by-case in practice. The vast majority of the time, we measure performance by picking a score​​: a number that we can compute for a trained classifier on some data with the property that the higher this number is the better the performance is. Accuracy, precision, recall, F1 are all scores in this sense. Other ones that you might be familiar with might include MSE (mean squared error), MAE (mean absolute error) for regression tasks, but be careful that these are numbers that are better the lower​​ they are, so we generally call them “error” or “loss”. . ROC is not a score in the above sense. First of all, it is not even a number, it is a curve on the x-y plane. To make it a number, we generally calculate the area under a ROC curve (AUC). Now this number does have the property that the higher it is, the better something is.. That “something” however is not ONE classifier. Recall that we construct the ROC curve for model that output a number that we can compare with a chosen threshold​​ to decide whether or not an observation is positive or negative. Unfortunately, we call that number a “score” as well, perhaps you can translate it as “confidence”: it is a number that is higher when the model is more sure that it is a positive observation. Upshot is, “trained model + chosen threshold = classifier”, different threshold different classifier. The ROC AUC is a way to find out the performance of the “trained model” part of the equation by looking at its performance across ALL​​ threshold. . So, higher AUC, better model (or more specifically, better TPR-FPR trade off can be made with the model). . |",
            "url": "https://edmundlth.github.io/MBUSA-ML-2022/questions/machine-learning/cross-validation/2022/08/05/week1-q-and-a.html",
            "relUrl": "/questions/machine-learning/cross-validation/2022/08/05/week1-q-and-a.html",
            "date": " • Aug 5, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Externel Materials of Interest",
            "content": "(UNDER CONSTRUCTION!!) . Fastai lectures: Lecture 6 about decision trees and random forests | explained.ai Statistics vs ML terms: https://explained.ai/statspeak/index.html | Gradient boosting: https://explained.ai/gradient-boosting/ | . | Dall-E 2 prompt book: https://pitch.com/v/DALL-E-prompt-book-v1-tmd33y | Craiyon: https://www.craiyon.com/ | Essay by Leo Breiman (you might know him for the Bagging meta-learning technique): Statistical Modelling: The Two Cultures | .",
            "url": "https://edmundlth.github.io/MBUSA-ML-2022/extra/machine-learning/2022/08/05/external-materials-of-interest.html",
            "relUrl": "/extra/machine-learning/2022/08/05/external-materials-of-interest.html",
            "date": " • Aug 5, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Week 1 workshop discussions",
            "content": ". Summary . Some main take away from the workshop discussion: . We are | . This is the handwritten note that correspond loosely to what was written on the whiteboard in class. . import numpy as np import pandas as pd import matplotlib.pyplot as plt import io . . UNDER CONSTRUCTION!!! . from sklearn.datasets import load_iris X, y = load_iris(return_X_y=True) print(&quot;There are {} instances and {} features&quot;.format(X.shape[0], X.shape[1])) print(&quot;There are {} classes&quot;.format(np.unique(y).size)) . Let&#39;s see how cross-validation works in scikit-learn. $k$-fold cross-validation is implemented in sklearn.model_selection.KFold. . We start by instantiating a KFold instance, which takes the number of splits ($k$) as an argument. You can also specify whether to randomly shuffle the data before partitioning into folds (it&#39;s safest to set this to True). . We then call the split method on the iris data set and carry out some operations for each round in a for-loop. Note that the train and test arrays for each round are indices into the original data. We can access the feature vectors/labels using array indexing. . from sklearn.model_selection import KFold kf = KFold(n_splits=2, shuffle=True) for train, test in kf.split(X): print(&quot;--&quot;) print(&quot;Items in train:&quot;, train) print(&quot;Items in test:&quot;, test) . Below we estimate the accuracy of a logistic regression classifier using a held-out test set. . from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) clf = LogisticRegression() clf.fit(X_train, y_train) print(&quot;Accuracy on held-out test set:&quot;, clf.score(X_test, y_test)) .",
            "url": "https://edmundlth.github.io/MBUSA-ML-2022/machine-learning/workshop/2022/08/03/week1-worksheet.html",
            "relUrl": "/machine-learning/workshop/2022/08/03/week1-worksheet.html",
            "date": " • Aug 3, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://edmundlth.github.io/MBUSA-ML-2022/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "This site collects extra materials used by the Machine Learning module in the Master of Business Analytics course at Melbourne Business School (MBS). Due to copyright reason, course materials won’t be directly posted to this site, but I will attempt to capture a good summary of the topics discussed in class. If you are a student of the course, feel free to ask questions by commenting on each post and I will try to answer. If I take too long to reply (or I just missed it), perhaps emailing me might still be the better option. . This site is generated by fastpages, which is a blogging platform that natively support Jupyter notebooks. .",
          "url": "https://edmundlth.github.io/MBUSA-ML-2022/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://edmundlth.github.io/MBUSA-ML-2022/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}