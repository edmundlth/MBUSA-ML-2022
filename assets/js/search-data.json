{
  
    
        "post0": {
            "title": "Week 2 Written Notes and Exercise Solutions",
            "content": "",
            "url": "https://edmundlth.github.io/MBUSA-ML-2022/pen-and-paper/machine-learning/2022/08/10/week2-written-exercise.html",
            "relUrl": "/pen-and-paper/machine-learning/2022/08/10/week2-written-exercise.html",
            "date": " • Aug 10, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Week 2 workshop discussions",
            "content": ". Workshop plan . We will first finish up NaiveBayes classifier from last week. We will do demo on this, illustrating Bayesian inference and the &quot;naive&quot; assumption. This will hopefully illustrate some aspect of information entropy as well. | Then we will talk about DecisionTreeClassifier. We will discuss this in detail. It is important to understand the &quot;splitting criterion&quot;. We will understand the basic concept of Entropy first. (Q1 written) Demo: number guessing game, probable event encoding. | Entropy as &quot;average surprise&quot;. We want &quot;additive surprise on independent events&quot;. | Entropy as a function of distribution. $H(p)$ | Entropy as a funciton of random variable. $H(X)$ | Entropy of a set. $H(A)$ | Conditional entropy. $H(Y | X)$ | . | Use entropy to define &quot;information gain&quot;. (Q2 written). | . | Next, we talk about &quot;ensembling&quot;. Example: &quot;crowd wisdom&quot;. | The need for classifiers to be &quot;as independent / diverse as possible&quot;. | Bagging: drawing with replacement. | Random Forest classifier. | . | Some coding exercise: Q3, Q4. We will see some &quot;preprocessing&quot; of data. | We will talk about what ML people call &quot;hyperparameters&quot;. Why &quot;hyper&quot;? Hyperparameters in random forest Q5. | . | If time permits, we will talk about: Any question you have. | Overfitting. | Some possibly useful external materials for better understanding. | . | Summary. What do you absolutely need to master from these very diverse materials? | import numpy as np import matplotlib.pyplot as plt import scipy from ipywidgets import interact . . . Bayesian inference demo . Below is a classic demonstration of Bayesian inference. The likelihood we set up will also illustrate the &quot;naive&quot; assumptions used in the NaiveBayes classifier. . Set up . A point $(x^*, y^*)$ is randomly chosen on the unit square $[0, 1] times [0, 1]$. Our task is to find out where that point is based on a sequence of relative direction of randomly generated points on the square. E.g. If $(x^*, y^*) = (0.5, 0.5)$ and the first randomly generated point is $(x_1, y_1) = (0.9, 0.3)$ then we obtain one data point of $SE$ since $(x, y)$ is south east of $(x^*, y^*)$. . Likelihood function . The likelihood of observing $N$ north calls and $E$ east calls in a sequence of $n$ points is given by . $$ begin{align*} L(x, y) &amp;= p(N, E | x^*, y) &amp;= p(N | y) p(E | x) &amp;= binom{n}{N}(1-y)^N y^{n - N} binom{n}{E} (1 - x)^E x^{n - E}. end{align*} $$Notice that . We are assuming that the points are generated uniformly at random. The set up did not tell us that, we are making a modelling assumption here. | In constructing the likelihood function above, we assumed that the North-South and East-West direction are independent. This is not true in general, this is the &quot;naive&quot; assumption, though in this case this follows from the much stronger first assumption above. | The likelihood function is usually thought of as a function of the information we wish to know, in this case the position of the point $P = (x^*, y^*)$, and the likelihood $L(x, y)$ is asking &quot;what is the probability of observing the sequence $ {NE, SE, dots }$ data if $P = (x, y)$&quot;. Crucially, this is NOT the probabiliy of $(x, y)$ in any sense of the word. To get the probability interpretation, we need Bayes formula. | . Bayes formula . $$ p(x, y | N, E) = frac{L(x, y) p(x, y)}{p(N, E)} $$where $p(x, y)$ is a prior and $p(N, E)$ is the evidence (while it is just a normalising constant, it is the term that tell us how likely our modelling assumption is. If the true point generator far from uniform, then evidence is going to be really low). . Demo . Run the follow cell in a Jupyter notebook and give different values of $N$ and $E$ (and control the total $n$ by giving corresponding $S$ and $W$) to see how the posterior changes and how the Maximum Likelihood estimate need not agree with what the posterior says. . def log_likelihood(north, south, east, west, x, y): x_term = east * np.log(1 - x) + west * np.log(x) y_term = north * np.log(1 - y) + south * np.log(y) return x_term + y_term MAX_N = 8 @interact( north=(0, MAX_N), south=(0, MAX_N), east=(0, MAX_N), west=(0, MAX_N), use_prior=True ) def plot_likelihood(north, south, east, west, use_prior): xlist = np.linspace(0.001, 0.999, 100) ylist = np.linspace(0.001, 0.999, 100) X, Y = np.meshgrid(xlist, ylist) Z = np.exp(log_likelihood(north, south, east, west, X, Y)) if use_prior: sigma = 0.5 prior = ( np.exp(-(X - 0.5)**2 / (2 * sigma**2)) * np.exp(-(Y - 0.5)**2 / (2 * sigma**2)) ) else: prior = 1 Z *= prior fig, ax = plt.subplots(1, 1, figsize=(8, 8)) ax.contourf(X, Y, Z, levels=20, cmap=&quot;hot_r&quot;) ax.vlines([0.5], ymin=0, ymax=1, linestyle=&quot;dotted&quot;) ax.hlines([0.5], xmin=0, xmax=1, linestyle=&quot;dotted&quot;) # x_index, y_index = np.unravel_index(np.argmax(Z, axis=None), Z.shape) # x_mle = xlist[x_index] # y_mle = ylist[y_index] x_mle = west / max((east + west), 1) y_mle = south / max(north + south, 1) ax.plot([x_mle], [y_mle], &quot;o&quot;, markersize=10, color=&quot;orange&quot;) ax.set_title(f&quot;Posterior contour. nMaximum Likelihood at: ({x_mle:.2f}, {y_mle:.2f})&quot;) . . Decision tree . An illustration of a &quot;regression tree&quot;: See sklearn docs .",
            "url": "https://edmundlth.github.io/MBUSA-ML-2022/machine-learning/workshop/decision-tree/random-forest/information-theory/2022/08/10/week2-worksheet.html",
            "relUrl": "/machine-learning/workshop/decision-tree/random-forest/information-theory/2022/08/10/week2-worksheet.html",
            "date": " • Aug 10, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Q&A (Week 2)",
            "content": "In terms of hyperparameters, I think you mentioned that X=(X1, X2, …) is part of hyperparameters. Does this mean the train-test split is part of the hyperparameter? . Sorry that confused you. I did not meant to imply that X = (X1, X2, …) are hyperparameters, they are not parameters of any kind. Depending on what your notation meant, they are either features, i.e. each observation (data you collect) constitute a row in a data frame of the form (X, y) with X = (X1, X2, …, Xn) being the independent variables, i.e. the features. . I was giving the example that one could do “feature selection” in which one chooses a subset of N best features according to some criteria (e.g. correlation of X_i with the response y). The size of that subset, N, can be a hyperparameter. Meaning it is something you choose BEFORE you call model.fit()​. If your model is a linear model, then the model itself has parameters (the linear coefficients in this case). Each parameter will produce a different “guess” or “hypothesis” that explains the relationship between X and y and the role of model.fit()​ is to find the best parameter according to a given goodness criterion (accuracy, mean squared error etc). Each different hyperparameter (e.g. N above) will also produce a different hypothesis whose goodness can be measure with any given data. BUT, it is NOT optimised (or “trained”) by model.fit()​. It is instead chosen before calling .fit()​ and usually changes which set of parameter .fit()​ can even affect. In that sense, it is a parameter that control other parameters, hence the name “hyperparameter”. . | Regarding the K-fold stratified cross-validation, some sources mention it improves accuracy. Is this referring to higher accuracy on the training set, or higher accuracy for the test set as well? Cross-validation (of any sort. K-fold, leave-one-out, random sampling) is a method to evaluate​​ out-of-sample accuracy (or any other performance measures). It doesn’t, by itself improves anything. However, by doing cross-validation, one can get information about performance of the model in the future. So, if you do cross-validation across different classifiers, then you can pick the one with the best cross-validated scores to ensure the final classifiers you choose has the best accuracy among all the classifiers you tested. . Also, to use exaggerated language, we NEVER evaluate performance on training set. While we do evaluate accuracy or other performance scores on training set, it is for the purpose of “fitting” the model: to find a parameter, configuration (e.g. decision tree branches) that do well to explain the training data. The score is used for this search, but we never believe that the scores computed during training will be indicative of future performance. We always evaluate how well our model might do on future data – how well our model “generalise” – using data it has never seen during training. This out-of-sample data set can either be a test set (data set aside specifically for this purpose) or we go through a cross-validation process. . | For the decision tree, I think in class we were told to stop splitting when “instances at root have the same class”. Do we need to consider whether to split the continuous features before or after the categorical features when constructing the decision tree? (since taking a different threshold may affect the number of classes after the split, affect the Information Gain calculated, and that may affect when we stop splitting). Or does the threshold or order have no impact on the final model? . That is indeed a subtle thing. Let’s recall what we do for the case where all the features are categorical and the response y is also categorical: at every node, we split on the feature that has the highest information gain. In this case, the ordering of features doesn’t matter since we evaluate InfoGain on every feature and take the best. . Things becomes slightly complicated when continuous variables are involved, but the overall story doesn’t really change: We still construct a way to evaluate “goodness of split” for each feature, evaluate this on all feature and then split on the best. The only that changed is how we evaluate this “goodness of split” on continuous features. So, the order still doesn’t matter, we still evaluate on everything and pick the best. . How we evaluate goodness of split depends on the type of feature and the type of response: . Categorical input -&gt; categorical output. This is the case discussed in lecture and workshop, we evaluate information gain or similar purity criteria. | Categorical input -&gt; continuous output. We can still split on this feature, but now we need to extend the idea of purity that we evaluate on each node to continuous variable. To motivate this a little with an extremely nice situation. We have the following data [(A, 5), (B, 1000), (B, 1050), (A, 7)]. It is clear that when the input category is A, the output is very small and when it is B it is large. The decision rule “if input is A, predict output = 6, else predict 1025”, where those prediction values are the average, this look like a good rule. In general, if we split on the feature values and we use the “average” of the output in each split as our prediction, then we evaluate how good our split is by looking expected error that kind of prediction will make. In our simple example, not splitting on that feature will give the prediction of “5 + 7 + 1000 + 1050 / 4 = 515.5” which is far from all the values we have seen, while if we do split and predict base on A or B, we improve to predicting 6 (which is going to be close to any values in the A branch) and 1025 which is close to any values in the B branch. So in general, we still have a formulate that is similar in spirit to the InfoGain formula: “error of prediction at parent node - average error across children node”. | Continuous input -&gt; categorical output. One way to handle this is to use a threshold on the input value. If we pick a threshold T, then splitting based on whether input &lt; T​ effectively turn this into case 1 above: categorical (binary) input -&gt; categorical output. Its just that there are infinitely many T to choose from. Well, we can just choose the “best T”, meaning find the T where the resulting split has best purity score. And then the goodness of split from that T is then the goodness of split for this feature overall. Choosing the best T from infinitely many might seem impossible, but, this is a very typical situation that just turn into an optimisation problem. E.g. find out the function InfoGain(T) and use calculus to solve for the optimal T. | Continuous input -&gt; continuous output. This is now a regression problem. One way to handle this is to do the same thing as case 2 above, we have a categorical (binary now) input -&gt; continuous output. And the issue of which threshold to choose is again down to issue of optimisation like case 3. | All the above have variation with various strength and weaknesses, but perhaps understand the reasoning behind these methods first might help. See sklearn documentation on decision tree (classifiers and regressors) for reference. . | For ranking-based feature selection of discrete valued features using information gain, is the mutual information calculated based on the original data set (from the top of tree without any split)? . Short answer: Mutual information (or correlation, or any such scores used for feature selection) is evaluated on the “train” data both the train-test split and train-validation-test split. . Long answer: First thing first, feature selection is something you can do regardless of what model you end up using, so this is not about decision tree per se. You might select features you think are best and feed it into DecisionTree, RandomForest, linear models or anything you wish. . The reason we shouldn’t use validation or test data to do feature selection is related to why we don’t use training data to evaluate performance. Feature selection is something you do as part of training: the selected features is going to be used by model.fit()​ to find best parameter. If you use the validation or test data to find such a feature set and then evaluate performance on data you already use to inform how your model is constructed, the resulting performance is going to be overly optimistic. . | I am confused between wrapper-based feature selection and embedded feature selection, as they both seem to have feature selection in parallel to training. Is the main difference that the “mechanism” in embedded feature selection needs to be explicitly defined in the algorithm (like decision tree), while it is considered unknown in the wrapper-based feature selection before the model is trained? . As far as I can tell, your description is correct. The embedded rely on “performance measure” that is built into the model you choose whereas wrapper one is independent on what model you use to evaluate performance. I am not sure these are standard terminology as I have not seen it much … I am going to point you to this site for this question. . | In the Bagging topic, the slide mentioned to use “cross-validation to evaluate its out-of-sample data”. Does it mean that the accuracy will be calculated over the out-of-sample data each time even if the size is different? And when it says bagging “improves unstable classifiers by reducing variance”, I am confused about what this improvement is relative to. Is this talking about improvement in the performance after taking majority vote or averaging compared to the performance from base classifiers which are unstable? . Yes. Remember, the main point of cross-validation is that we evaluate performance on samples not used to train the model. Evaluating on 1 sample the model didn’t see during training is still a way to predict performance, but the value you get out is going to have a high variance compare to evaluating performance on lots of samples and take average. This is just using phenomena typified in the Central limit theorem: if one sample has variance S^2, then averaging over N independent samples has variance S^2 / N. . And yes again, the improvement comes from averaging unstable (high variance) base estimators. Bagging is again using central limit theorem-like phenomena, if you have N independent prediction, you can reduce your prediction variance by 1/N. Although we don’t have guaranteed independence as needed in central limit theorem, we use a lot of tricks (random subsampling training data, random subsampling feature set etc) to make the individual base estimators as “independent” as possible. So, although we won’t get full 1/N reduction in variance, there will still be some​ improvement over individuals. . |",
            "url": "https://edmundlth.github.io/MBUSA-ML-2022/questions/machine-learning/cross-validation/2022/08/10/week2-q-and-a.html",
            "relUrl": "/questions/machine-learning/cross-validation/2022/08/10/week2-q-and-a.html",
            "date": " • Aug 10, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Q&A (Week 1)",
            "content": "Under the Holdout method in the “Sampling for evaluation” section in the worksheet, does the “test set or holdout set” here refer to the validation set or test set? Is validation set relevant to both Holdout method and Cross-validation, or is it only relevant to Cross-validation? . Simple answer: It is referring to the test set. . Complicated answer: Validation set and test set, while referring to different subsets of data, play very similar role. They are both “hold out” or “set aside” part of our full dataset that is exclusively used to find out how well our classifier is performing without using data the classifier has already seen during its training process. . Long answer: The main thing to understand here is that we want to predict how well our classifier will perform and to do that we need to evaluate​​ our classifier on some data. But to get a good estimate of performance, we should not evaluate the model on data it has already seen in the training process because the training process involve trying to do as well as possible on the training data so of course it is expected to do well on them. We need “unseen” / “hold out”/ “set aside” / “out of sample” / “TEST” data. The word “validation” and “test” is basically synonyms, but we use different words just to signal that we are doing the test in slightly different context. See, we might want to know how well our classifier perform for different purposes​​: There is one obvious one: to get a final performance report, the one that you use to boast in a paper, to your boss or to convince customers to use your classifier. This is the number you use to predict how well your classifier will actually perform in the future​​. We often call the set of data that generate this number the “test set”. (but really, this is just convention by this point). One other reason you might want to evaluate performance is to do model selection​​ (notice that KFold​ is implemented in the model_selection​ module of sklearn​). This is the scenario where you have multiple classifier that has already been trained and you want compare​​ their performance. Well, you can evaluate their performance on data not used in any of their training process and choose the classifier that perform best. This set of data used to evaluate performance is often call the “test set” as well. BUT!!​​ This process of selecting the best classifier from multiple classifiers to come up with one ultimate classifier IS PART OF TRAINING​​: it is a process that uses data to find a classifier that we hope perform well in future instance of similar data. To know the actual performance of that selected classifier, we can’t use the same data that we used to select it (same reasoning, it is selected to do well on that data, so of course it will do well). We need to evaluate on another test set. Now we have a linguistic problem of using the same term “test set” referring on two physically different (different values, store in physically different location) thing. So we just change the word to a synonymous one, which is why we call it the Validation set​​. The reason my “simple answer” just says that it is the “test set” is because we haven’t run into this issue of needing to “test” / evaluation for different purpose yet and “test” is the more common / default / general word to use (it is the default word in many ML libraries). . Analogy: We use “mid sem test”, “final test”, “exam”, “quiz” to refer the process of evaluating student performance for different purpose. and different context We could’ve use “model selection testing set” to disambiguate, but “validation” is already widely used. . | Under the Cross-validation evaluation method, when it says “test set” or “testing” in the definitions, does the “test” here refer to validation set? . Following the story we outlined above, “testing” here refer to the process of evaluation classifier performance on data it hasn’t seen during training. We do not need to make the distinction between test / validation here because we are talking about that performance evaluation process in general. . Cross-validation is just a method of splitting full data set in “data to train model” and “data unseen during training” multiple time so that we can train on multiple different training set and evaluate their performance. This is usually known as the “train-test split”. The idea is to get different independently trained model and independently evaluate their performance on independent data so that, when we take average of these independent evaluation, it is a better prediction of actual performance. Average of independent samples from the same distribution is a better prediction of the mean is a basic result from probability, but just how “independent” these evaluation are is up for debate, but we are only applying it as a general wisdom that taking average = good. . | In the Random sub-sampling method, the description says “each train/test split is drawn with replacement from the original data set”. What does the part “with replacement from the original data set” refer to? . Simple answer: This is just one way to come up with the train-test split. And it is talking about a process that comes up with training and testing set using a process like the following. Suppose full data set is of size 10. And you want to get 5 data point in training set and 3 in testing set (exact number not important for now). We can generate 5 random numbers, e.g. [2, 8, 9, 1, 1] and 3 more [5, 7, 8] and put into our training set the data points with index [2, 8, 9, 1, 1] and our test set the data point with index [5, 7, 8]. . Long answer: “Random draw with replacement” is a term used in probability theory. It means we generate a sample of N things by reaching into a bag, randomly picking up something, record what it is and return it into the bag​​, and repeat this N times. Notice that the “replacement” part makes it possible to pick up something you have picked up before (see the index 1 repeated twice above and the index 8 repeated in train and test set). That is a behaviour you would not get if you are doing “random draw without​​ replacement”. Wait ….. isn’t this doing test evaluation on data you might have seen before …… Good point. But if your data set is large enough, that is going to be rare and it turns out that it is not cheating if the distribution​​ of training and testing set is different, even if there might be repeated instances. But you are right though, “sampling without replacement” is a more common practice (most libraries that implement train_test_split, or k-fold cross-validation has a shuffle=true​ option, which is effectively doing sampling without​​ replacement). . How much do you need to care about this? If you haven’t mastered what train-test data are, what “model” means, what K-fold cross validation is, and know a few models and how they are trained and tested and figure out what constitute good “performance measure”, maybe leave this alone first. Just know that this is one option to get train-test set, it is not used particularly often and the theory of why it is valid is perhaps not very important to you right now. . | In the Leave-one-out cross-validation method (see worksheet), it says “n is the number of instances in the data set”. Is this “data set” here referring to the “train and validation set”? . Simple answer: “Data set” refer to full data set, i.e. all the data you have. . Long answer: “Data set” refers to all the data you have to perform training and testing. Remember that we might be talking about testing in different context here. So might not be doing model selection and we just want to train the model and find out how well it does, then we are talking about all the data you have. And if we are talking about a process with selection, then we might already have a testing data set aside for testing the ultimate model, but we might do cross-validation to split the remaining​​ data multiple times, then n = size of remaining data​. . Again, K-Fold, Leave-one-out (which is just K-fold with K selected so that test set is always of size 1) or any such splitting method is there to split any data set, you decide what data set to split and why. . | I might have said something like “ROC is not a score”. Why? What about precision_score, recall_score, and f1_score? . I think I said something like “ROC is not a score like what the score I wrote on the whiteboard meant”. I regretted saying that and I regret picking that word “score” to write on the whiteboard. You might have notice that I use the word “performance” a lot above and I think that is closer to what I meant. Throughout the above questions and answers, I didn’t mention how​​ this performance is to be measured, and indeed that is something we determine case-by-case in practice. The vast majority of the time, we measure performance by picking a score​​: a number that we can compute for a trained classifier on some data with the property that the higher this number is the better the performance is. Accuracy, precision, recall, F1 are all scores in this sense. Other ones that you might be familiar with might include MSE (mean squared error), MAE (mean absolute error) for regression tasks, but be careful that these are numbers that are better the lower​​ they are, so we generally call them “error” or “loss”. . ROC is not a score in the above sense. First of all, it is not even a number, it is a curve on the x-y plane. To make it a number, we generally calculate the area under a ROC curve (AUC). Now this number does have the property that the higher it is, the better something is.. That “something” however is not ONE classifier. Recall that we construct the ROC curve for model that output a number that we can compare with a chosen threshold​​ to decide whether or not an observation is positive or negative. Unfortunately, we call that number a “score” as well, perhaps you can translate it as “confidence”: it is a number that is higher when the model is more sure that it is a positive observation. Upshot is, “trained model + chosen threshold = classifier”, different threshold different classifier. The ROC AUC is a way to find out the performance of the “trained model” part of the equation by looking at its performance across ALL​​ threshold. . So, higher AUC, better model (or more specifically, better TPR-FPR trade off can be made with the model). . |",
            "url": "https://edmundlth.github.io/MBUSA-ML-2022/questions/machine-learning/cross-validation/2022/08/05/week1-q-and-a.html",
            "relUrl": "/questions/machine-learning/cross-validation/2022/08/05/week1-q-and-a.html",
            "date": " • Aug 5, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "External Materials of Interest",
            "content": "Fastai lectures: Lecture 6 about decision trees and random forests | explained.ai Statistics vs ML terms: https://explained.ai/statspeak/index.html | Gradient boosting: https://explained.ai/gradient-boosting/ | . | Dall-E 2 prompt book: https://pitch.com/v/DALL-E-prompt-book-v1-tmd33y | Craiyon: https://www.craiyon.com/ | Essay by Leo Breiman (you might know him for the Bagging meta-learning technique): Statistical Modelling: The Two Cultures | .",
            "url": "https://edmundlth.github.io/MBUSA-ML-2022/extra/machine-learning/2022/08/05/external-materials-of-interest.html",
            "relUrl": "/extra/machine-learning/2022/08/05/external-materials-of-interest.html",
            "date": " • Aug 5, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Week 1 Written Notes and Exercise Solutions",
            "content": "",
            "url": "https://edmundlth.github.io/MBUSA-ML-2022/pen-and-paper/machine-learning/2022/08/03/week1-written-notes.html",
            "relUrl": "/pen-and-paper/machine-learning/2022/08/03/week1-written-notes.html",
            "date": " • Aug 3, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "This site collects extra materials used by the Machine Learning module in the Master of Business Analytics course at Melbourne Business School (MBS). Due to copyright reason, course materials won’t be directly posted to this site, but I will attempt to capture a good summary of the topics discussed in class. If you are a student of the course, feel free to ask questions by commenting on each post and I will try to answer. If I take too long to reply (or I just missed it), perhaps emailing me might still be the better option. . This site is generated by fastpages, which is a blogging platform that natively support Jupyter notebooks. .",
          "url": "https://edmundlth.github.io/MBUSA-ML-2022/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://edmundlth.github.io/MBUSA-ML-2022/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}